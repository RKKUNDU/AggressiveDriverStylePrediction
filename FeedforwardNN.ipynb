{"cells":[{"metadata":{},"cell_type":"markdown","source":"# import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error,accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import GridSearchCV","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data preprocessing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read dataset\ndf = pd.read_csv('../input/alldriverdataset/25_features_in_one_row.csv')\n\n# Drop rows with NA values\ndf = df.dropna(axis = 0)\n\n# Divide dataset into input features, output labels\nX, y = df.drop(['Unnamed: 0', 'DrivingStyle', 'DriverID'], axis = 1), df['DrivingStyle']\n\n# encoding categorical values\nX = pd.get_dummies(X)\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\n# split dataset into train & test\n#X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.40, shuffle = True, random_state=42)\n#X_dev, X_test, y_dev, y_test = train_test_split(X_rem, y_rem, test_size=0.50, shuffle = True, random_state=42)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle = True, random_state=42)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MLP =MLPClassifier(hidden_layer_sizes = (100,100,100),activation = 'relu', solver = 'sgd',alpha = 0.1, batch_size = 128,learning_rate = 'constant',learning_rate_init = 1e-4,max_iter = 250,shuffle = False,momentum = 0, nesterovs_momentum = False,early_stopping = False,validation_fraction = 0, verbose  = True)\n#MLP = MLPClassifier(hidden_layer_sizes = (100,100),activation = 'relu', solver = 'adam',alpha = 0.1, batch_size = 128,learning_rate = 'adaptive',learning_rate_init = 1e-4,max_iter = 500,shuffle = False,momentum = 0, nesterovs_momentum = False,early_stopping = False,validation_fraction = 0, verbose  = True)\n#MLP =MLPRegressor(hidden_layer_sizes = (64,64,64),activation = 'relu', solver = 'sgd',alpha = 0, batch_size = 128,learning_rate = 'constant',learning_rate_init = 1e-6,max_iter = 500,shuffle = False,momentum = 0, nesterovs_momentum = False,early_stopping = False,validation_fraction = 0, verbose  = True)\nMLP.fit(X_train, y_train)\n#predictions_train = MLP.predict(X_train)\npredictions_test = MLP.predict(X_test)\n\n#prediction_dev_driver = np.vstack((Xtot_dev[:,0],predictions_dev)).T\nprint(confusion_matrix(y_test,predictions_test))\nprint(classification_report(y_test,predictions_test))","execution_count":7,"outputs":[{"output_type":"stream","text":"Iteration 1, loss = 1.31294284\nIteration 2, loss = 1.29314188\nIteration 3, loss = 1.27564465\nIteration 4, loss = 1.26011229\nIteration 5, loss = 1.24625037\nIteration 6, loss = 1.23376709\nIteration 7, loss = 1.22245612\nIteration 8, loss = 1.21213651\nIteration 9, loss = 1.20265261\nIteration 10, loss = 1.19388725\nIteration 11, loss = 1.18575704\nIteration 12, loss = 1.17815809\nIteration 13, loss = 1.17101638\nIteration 14, loss = 1.16427961\nIteration 15, loss = 1.15791494\nIteration 16, loss = 1.15188620\nIteration 17, loss = 1.14615232\nIteration 18, loss = 1.14068618\nIteration 19, loss = 1.13548186\nIteration 20, loss = 1.13052092\nIteration 21, loss = 1.12576965\nIteration 22, loss = 1.12121016\nIteration 23, loss = 1.11683755\nIteration 24, loss = 1.11263808\nIteration 25, loss = 1.10859768\nIteration 26, loss = 1.10470938\nIteration 27, loss = 1.10096264\nIteration 28, loss = 1.09734956\nIteration 29, loss = 1.09386120\nIteration 30, loss = 1.09049488\nIteration 31, loss = 1.08724163\nIteration 32, loss = 1.08409390\nIteration 33, loss = 1.08104956\nIteration 34, loss = 1.07810397\nIteration 35, loss = 1.07525156\nIteration 36, loss = 1.07249272\nIteration 37, loss = 1.06982505\nIteration 38, loss = 1.06724468\nIteration 39, loss = 1.06474386\nIteration 40, loss = 1.06231712\nIteration 41, loss = 1.05996109\nIteration 42, loss = 1.05767444\nIteration 43, loss = 1.05545422\nIteration 44, loss = 1.05329686\nIteration 45, loss = 1.05119992\nIteration 46, loss = 1.04916004\nIteration 47, loss = 1.04717550\nIteration 48, loss = 1.04524387\nIteration 49, loss = 1.04336327\nIteration 50, loss = 1.04153149\nIteration 51, loss = 1.03974624\nIteration 52, loss = 1.03800762\nIteration 53, loss = 1.03631537\nIteration 54, loss = 1.03466696\nIteration 55, loss = 1.03305822\nIteration 56, loss = 1.03149004\nIteration 57, loss = 1.02996007\nIteration 58, loss = 1.02846465\nIteration 59, loss = 1.02700316\nIteration 60, loss = 1.02557533\nIteration 61, loss = 1.02418048\nIteration 62, loss = 1.02281761\nIteration 63, loss = 1.02148387\nIteration 64, loss = 1.02017495\nIteration 65, loss = 1.01889226\nIteration 66, loss = 1.01763650\nIteration 67, loss = 1.01640525\nIteration 68, loss = 1.01519810\nIteration 69, loss = 1.01401360\nIteration 70, loss = 1.01285212\nIteration 71, loss = 1.01171323\nIteration 72, loss = 1.01059577\nIteration 73, loss = 1.00949849\nIteration 74, loss = 1.00842014\nIteration 75, loss = 1.00736213\nIteration 76, loss = 1.00632432\nIteration 77, loss = 1.00530752\nIteration 78, loss = 1.00430900\nIteration 79, loss = 1.00332914\nIteration 80, loss = 1.00236735\nIteration 81, loss = 1.00142239\nIteration 82, loss = 1.00049334\nIteration 83, loss = 0.99957995\nIteration 84, loss = 0.99868072\nIteration 85, loss = 0.99779593\nIteration 86, loss = 0.99692548\nIteration 87, loss = 0.99606934\nIteration 88, loss = 0.99522524\nIteration 89, loss = 0.99439349\nIteration 90, loss = 0.99357518\nIteration 91, loss = 0.99276864\nIteration 92, loss = 0.99197236\nIteration 93, loss = 0.99118676\nIteration 94, loss = 0.99040914\nIteration 95, loss = 0.98964033\nIteration 96, loss = 0.98888248\nIteration 97, loss = 0.98813467\nIteration 98, loss = 0.98739666\nIteration 99, loss = 0.98666748\nIteration 100, loss = 0.98594652\nIteration 101, loss = 0.98523384\nIteration 102, loss = 0.98452950\nIteration 103, loss = 0.98383404\nIteration 104, loss = 0.98314729\nIteration 105, loss = 0.98246770\nIteration 106, loss = 0.98179607\nIteration 107, loss = 0.98113170\nIteration 108, loss = 0.98047363\nIteration 109, loss = 0.97982307\nIteration 110, loss = 0.97918011\nIteration 111, loss = 0.97854439\nIteration 112, loss = 0.97791568\nIteration 113, loss = 0.97729423\nIteration 114, loss = 0.97667984\nIteration 115, loss = 0.97607199\nIteration 116, loss = 0.97547039\nIteration 117, loss = 0.97487434\nIteration 118, loss = 0.97428374\nIteration 119, loss = 0.97369830\nIteration 120, loss = 0.97311803\nIteration 121, loss = 0.97254361\nIteration 122, loss = 0.97197496\nIteration 123, loss = 0.97141260\nIteration 124, loss = 0.97085502\nIteration 125, loss = 0.97030149\nIteration 126, loss = 0.96975353\nIteration 127, loss = 0.96921061\nIteration 128, loss = 0.96867261\nIteration 129, loss = 0.96813882\nIteration 130, loss = 0.96760954\nIteration 131, loss = 0.96708413\nIteration 132, loss = 0.96656291\nIteration 133, loss = 0.96604593\nIteration 134, loss = 0.96553307\nIteration 135, loss = 0.96502467\nIteration 136, loss = 0.96452063\nIteration 137, loss = 0.96402086\nIteration 138, loss = 0.96352467\nIteration 139, loss = 0.96303290\nIteration 140, loss = 0.96254527\nIteration 141, loss = 0.96206143\nIteration 142, loss = 0.96158227\nIteration 143, loss = 0.96110680\nIteration 144, loss = 0.96063487\nIteration 145, loss = 0.96016603\nIteration 146, loss = 0.95970038\nIteration 147, loss = 0.95923889\nIteration 148, loss = 0.95878166\nIteration 149, loss = 0.95832727\nIteration 150, loss = 0.95787639\nIteration 151, loss = 0.95742994\nIteration 152, loss = 0.95698742\nIteration 153, loss = 0.95654821\nIteration 154, loss = 0.95611200\nIteration 155, loss = 0.95567899\nIteration 156, loss = 0.95524869\nIteration 157, loss = 0.95482150\nIteration 158, loss = 0.95439792\nIteration 159, loss = 0.95397702\nIteration 160, loss = 0.95355939\nIteration 161, loss = 0.95314488\nIteration 162, loss = 0.95273326\nIteration 163, loss = 0.95232427\nIteration 164, loss = 0.95191834\nIteration 165, loss = 0.95151607\nIteration 166, loss = 0.95111666\nIteration 167, loss = 0.95072018\nIteration 168, loss = 0.95032680\nIteration 169, loss = 0.94993656\nIteration 170, loss = 0.94954904\nIteration 171, loss = 0.94916341\nIteration 172, loss = 0.94878052\nIteration 173, loss = 0.94840076\nIteration 174, loss = 0.94802354\nIteration 175, loss = 0.94764819\nIteration 176, loss = 0.94727448\nIteration 177, loss = 0.94690330\nIteration 178, loss = 0.94653476\nIteration 179, loss = 0.94616814\nIteration 180, loss = 0.94580399\nIteration 181, loss = 0.94544220\nIteration 182, loss = 0.94508293\nIteration 183, loss = 0.94472656\nIteration 184, loss = 0.94437221\nIteration 185, loss = 0.94402020\nIteration 186, loss = 0.94367043\nIteration 187, loss = 0.94332276\nIteration 188, loss = 0.94297785\nIteration 189, loss = 0.94263525\nIteration 190, loss = 0.94229438\nIteration 191, loss = 0.94195605\nIteration 192, loss = 0.94161996\nIteration 193, loss = 0.94128581\nIteration 194, loss = 0.94095384\nIteration 195, loss = 0.94062355\nIteration 196, loss = 0.94029505\nIteration 197, loss = 0.93996797\nIteration 198, loss = 0.93964303\nIteration 199, loss = 0.93931956\nIteration 200, loss = 0.93899796\nIteration 201, loss = 0.93867813\nIteration 202, loss = 0.93836055\nIteration 203, loss = 0.93804552\nIteration 204, loss = 0.93773241\nIteration 205, loss = 0.93742125\nIteration 206, loss = 0.93711176\nIteration 207, loss = 0.93680370\nIteration 208, loss = 0.93649701\nIteration 209, loss = 0.93619288\nIteration 210, loss = 0.93589064\nIteration 211, loss = 0.93559092\nIteration 212, loss = 0.93529280\nIteration 213, loss = 0.93499661\nIteration 214, loss = 0.93470182\nIteration 215, loss = 0.93440905\nIteration 216, loss = 0.93411738\nIteration 217, loss = 0.93382756\nIteration 218, loss = 0.93353895\nIteration 219, loss = 0.93325222\nIteration 220, loss = 0.93296789\nIteration 221, loss = 0.93268481\nIteration 222, loss = 0.93240342\nIteration 223, loss = 0.93212317\nIteration 224, loss = 0.93184486\nIteration 225, loss = 0.93156788\nIteration 226, loss = 0.93129265\nIteration 227, loss = 0.93101848\nIteration 228, loss = 0.93074619\nIteration 229, loss = 0.93047543\nIteration 230, loss = 0.93020675\nIteration 231, loss = 0.92993932\nIteration 232, loss = 0.92967395\nIteration 233, loss = 0.92940917\nIteration 234, loss = 0.92914578\nIteration 235, loss = 0.92888309\nIteration 236, loss = 0.92862175\nIteration 237, loss = 0.92836087\nIteration 238, loss = 0.92810094\nIteration 239, loss = 0.92784175\nIteration 240, loss = 0.92758431\nIteration 241, loss = 0.92732784\nIteration 242, loss = 0.92707316\nIteration 243, loss = 0.92682012\nIteration 244, loss = 0.92656880\nIteration 245, loss = 0.92631903\nIteration 246, loss = 0.92607024\nIteration 247, loss = 0.92582323\nIteration 248, loss = 0.92557779\nIteration 249, loss = 0.92533412\nIteration 250, loss = 0.92509151\n[[ 197  265  141]\n [  85 1068  240]\n [  93  207  540]]\n              precision    recall  f1-score   support\n\n         1.0       0.53      0.33      0.40       603\n         2.0       0.69      0.77      0.73      1393\n         3.0       0.59      0.64      0.61       840\n\n    accuracy                           0.64      2836\n   macro avg       0.60      0.58      0.58      2836\nweighted avg       0.63      0.64      0.63      2836\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hidden Layer Sizes\nhidden_layer_sizes = [(100),(100,100),(100,100,100),(50),(50,50),(50,50,50)]\n#Solver\nsolver = ['sgd', 'adam']\n# Maximum number of levels in tree\n# max_depth = [2,4]\nbatch_size = [128,256]\n#alpha = [0.1,0.01,0.001]\nlearning_rate = ['adaptive']\n\nlearning_rate_init = [0.01,0.001,0.0001]\n# Minimum number of samples required to split a node\n# min_samples_split = [2, 5]\n# Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2]\n\n# Method of selecting samples for training each tree\n#bootstrap = [True, False]\n\nparam_grid = {'hidden_layer_sizes':hidden_layer_sizes,\n               'solver': solver,\n                'batch_size': batch_size,\n                'learning_rate': learning_rate,\n     #            'alpha': alpha,\n               'learning_rate_init': learning_rate_init}\n\nMLPC_Model =MLPClassifier()\n\nMLPC_Grid = GridSearchCV(estimator = MLPC_Model, param_grid = param_grid, cv = 5, verbose=2, n_jobs = 4)\n\nMLPC_Grid.fit(X_train, y_train)\n\nMLPC_Grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (f'Train Accuracy - : {MLPC_Grid.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {MLPC_Grid.score(X_test,y_test):.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MLPC_Grid.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}